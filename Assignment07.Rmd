---
title: "Assignment07"
author: "Kaylie Evans"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Primary Example Code from Chapter 2 of Text Mining with R:

### Introduction
The below code has been adapted from code in Text Mining with R: A Tidy Approach by Julia Silge and David Robinson, ch. 2. The comments come from this textbook, minimally altered to explain each of the code chunks below them. The structure of the code has a focus on various methods to sentiment analysis employing texts from Jane Austen's literary works as illustrative examples.


### Import Libraries
```{r import-libraries}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(reshape2)
```


### Sentiment Analysis with Inner Join
```{r sentanaly-inner-join}
#First, we need to take the text of the novels and convert the text to the tidy format using unnest_tokens(), just as we did in Section 1.3. Let’s also set up some other columns to keep track of which line and chapter of the book each word comes from; we use group_by and mutate to construct those columns.
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

#First, let’s use the NRC lexicon and filter() for the joy words. Next, let’s filter() the data frame with the text from the books for the words from Emma and then use inner_join() to perform the sentiment analysis. What are the most common joy words in Emma? Let’s use count() from dplyr.
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)


#We then use pivot_wider() so that we have negative and positive sentiment in separate columns, and lastly calculate a net sentiment (positive - negative).
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)



#Now we can plot these sentiment scores across the plot trajectory of each novel.
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```


### Comparing the Three Sentiment Dictionaries
```{r three-sentiment-dictionaries}
#First, let’s use filter() to choose only the words from the one novel we are interested in.
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice


#Let’s again use integer division (%/%) to define larger sections of text that span multiple lines, and we can use the same pattern with count(), pivot_wider(), and mutate() to find the net sentiment in each of these sections of text.
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

#Let’s bind the estimate of the net sentiment (positive - negative) in each chunk of the novel text for each sentiment lexicon together and visualize them
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

#Let’s look briefly at how many positive and negative words are in these lexicons.
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```


### Most common positive and negative words
```{r common-pos-neg-words}
#By implementing count() here with arguments of both word and sentiment, we find out how much each word contributed to each sentiment.
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

#Pipe straight into ggplot2
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)



#the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using bind_rows(). We could implement that with a strategy such as this.
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```


### Wordclouds
```{r wordclouds}
#Let’s look at the most common words in Jane Austen’s works as a whole again, but this time as a wordcloud
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))


#In other functions, such as comparison.cloud(), you may need to turn the data frame into a matrix with reshape2’s acast(). Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud(), this can all be done with joins, piping, and dplyr because our data is in tidy format.
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```


### Looking at Units Beyond Just Words
```{r units-beyond-words}
#Tokenize text into sentences, and it makes sense to use a new name for the output column in such a case.
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

#Look at one
p_and_p_sentences$sentence[2]

#Another option in unnest_tokens() is to split into tokens using a regex pattern. We could use this, for example, to split the text of Jane Austen’s novels into a data frame by chapter.
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())


#First, let’s get the list of negative words from the Bing lexicon. 
#Second, let’s make a data frame of how many words are in each chapter so we can normalize for the length of chapters. 
#Then, let’s find the number of negative words in each chapter and divide by the total words in each chapter. For each book, which chapter has the highest proportion of negative words?
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```




## Extending the Code:

### Introduction
The second half of this RMD works with a different corpus chosen which is a selection of books written by the Brontë family and incorporates one additional sentiment lexicon, SentiWordNet.


### Import Library
```{r guten-libraries}
library(gutenbergr)
library(lexicon)
library(sentimentr)
```


### Import Books
```{r import-bronte-books}
#find the indices of the gutenberg Bronte family books
bronte_books <- gutenberg_works() |>
            filter(str_detect(author,"Brontë")) 

#download the books
bronte <- gutenberg_download(bronte_books$gutenberg_id)

#add information from bronte_books
bronte <- left_join(bronte, bronte_books, by = "gutenberg_id")
```

### Sentiment Analysis and Graph
```{r sentanaly-inner-join-bronte}
#tidy text using unnest_tokens()
tidy_bronte <- bronte |>
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) |>
  ungroup() |>
  unnest_tokens(word, text)

#new lexicon sentiwordnet
data(hash_sentiment_senticnet)
force(data(hash_sentiment_senticnet))
hash_sentiment_senticnet <- hash_sentiment_senticnet |>
  mutate(
    word = x,
    sentiment = y,
    x = NULL,
    y = NULL
  )
head(hash_sentiment_senticnet)

#filter() for the anger words
swn_anger <- hash_sentiment_senticnet |>
  filter(word == "anger")

#what does the format look like
tidy_bronte |>
  filter(title == "Wuthering Heights") |>
  inner_join(swn_anger) %>%
  count(word, sort = TRUE)

#sentiment analysis
bronte_sentiment <- sentiment(
tidy_bronte$word,
polarity_dt = lexicon::hash_sentiment_senticnet,
valence_shifters_dt = lexicon::hash_valence_shifters,
hyphen = "",
amplifier.weight = 0.8,
n.before = 5,
n.after = 2,
question.weight = 1,
adversative.weight = 0.25,
neutral.nonverb.like = FALSE,
missing_value = 0,
retention_regex = "\\d:\\d|\\d\\s|[^[:alpha:]',;: ]"
)

#add an index field
tidy_bronte <- tidy_bronte |>
  mutate(element_id = c(1:544169))

#join on the columns from tidy_bronte
bronte_sentiment <- inner_join(bronte_sentiment, tidy_bronte, by = "element_id")
 

#plot these sentiment scores across the plot trajectory of each novel.
ggplot(bronte_sentiment, aes(x = element_id, y = sentiment, fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = "free_x")
```